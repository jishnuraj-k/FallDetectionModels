{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m5F5kqqnUf8i"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "def main():\n",
        "    # Google Drive folder URL\n",
        "    folder_url = \"https://drive.google.com/drive/folders/1HzLt5YkXbaNHpzMt-0BZj6FDGEQ7x6T4?usp=drive_link\"\n",
        "\n",
        "    # Use the gdown library with the --remaining-ok flag\n",
        "    gdown.download_folder(folder_url, quiet=True, use_cookies=False, remaining_ok=True)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Path to the folder containing the dataset in your local directory\n",
        "dataset_folder = \"research/\"\n",
        "\n",
        "# Path to the base output folder where the activity folders will be created\n",
        "output_folder = \"output/\"\n",
        "\n",
        "# List of activities\n",
        "activities = [\"fall\", \"walk\"]\n",
        "\n",
        "# Function to read accelerometer data from a text file\n",
        "def read_accelerometer_data(file_path):\n",
        "    with open(file_path, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    x_axis = []\n",
        "    y_axis = []\n",
        "    z_axis = []\n",
        "    for line in lines:\n",
        "        # Split the line using commas as the delimiter\n",
        "        data = line.strip().split(',')\n",
        "        # Take the first three values as x, y, and z-axis data\n",
        "        x, y, z = map(float, data[:3])\n",
        "        x_axis.append(x)\n",
        "        y_axis.append(y)\n",
        "        z_axis.append(z)\n",
        "\n",
        "    return x_axis, y_axis, z_axis\n",
        "\n",
        "# Function to plot and save accelerometer data as an image\n",
        "def save_accelerometer_plot(x_axis, y_axis, z_axis, output_file):\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure()\n",
        "    plt.plot(x_axis, label=\"X axis\")\n",
        "    plt.plot(y_axis, label=\"Y axis\")\n",
        "    plt.plot(z_axis, label=\"Z axis\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Acceleration\")\n",
        "    plt.title(\"Accelerometer Data\")\n",
        "    plt.legend()\n",
        "    plt.savefig(output_file)\n",
        "    plt.close()\n",
        "\n",
        "# Iterate through each activity folder\n",
        "for activity in activities:\n",
        "    activity_folder = os.path.join(dataset_folder, activity)\n",
        "\n",
        "    # Create a corresponding output folder for the activity\n",
        "    output_activity_folder = os.path.join(output_folder, activity)\n",
        "    os.makedirs(output_activity_folder, exist_ok=True)\n",
        "\n",
        "    # Get the list of text files in the activity folder and its subfolders\n",
        "    txt_files = glob.glob(os.path.join(activity_folder, \"**/*.txt\"), recursive=True)\n",
        "\n",
        "    # Iterate through each text file and generate graphs\n",
        "    for txt_file in txt_files:\n",
        "        # Read the accelerometer data from the text file\n",
        "        x_data, y_data, z_data = read_accelerometer_data(txt_file)\n",
        "\n",
        "        # Get the base name of the text file\n",
        "        base_name = os.path.basename(txt_file)\n",
        "\n",
        "        # Save the graph image in the output folder with a unique name\n",
        "        output_file = os.path.join(output_activity_folder, os.path.splitext(base_name)[0] + \".png\")\n",
        "        save_accelerometer_plot(x_data, y_data, z_data, output_file)\n",
        "        print(f\"File '{base_name}' processed and saved as '{os.path.basename(output_file)}'\")\n",
        "\n",
        "print(\"Graph images generated and saved successfully!\")\n",
        "\n",
        "# Split the generated graph images into training and testing datasets\n",
        "# Ratio of data to be allocated for training (0.8 = 80% training, 20% testing)\n",
        "train_ratio = 0.8\n",
        "\n",
        "# Create the training and testing folders\n",
        "train_folder = os.path.join(output_folder, \"train\")\n",
        "test_folder = os.path.join(output_folder, \"test\")\n",
        "os.makedirs(train_folder, exist_ok=True)\n",
        "os.makedirs(test_folder, exist_ok=True)\n",
        "\n",
        "# Iterate through each activity folder\n",
        "for activity in activities:\n",
        "    activity_folder = os.path.join(output_folder, activity)\n",
        "\n",
        "    # Create corresponding activity folders in the training and testing datasets\n",
        "    train_activity_folder = os.path.join(train_folder, activity)\n",
        "    test_activity_folder = os.path.join(test_folder, activity)\n",
        "    os.makedirs(train_activity_folder, exist_ok=True)\n",
        "    os.makedirs(test_activity_folder, exist_ok=True)\n",
        "\n",
        "    # Get the list of graph image files for the current activity\n",
        "    graph_images = os.listdir(activity_folder)\n",
        "\n",
        "    # Randomly shuffle the graph images\n",
        "    random.shuffle(graph_images)\n",
        "\n",
        "    # Calculate the split index based on the train_ratio\n",
        "    split_index = int(len(graph_images) * train_ratio)\n",
        "\n",
        "    # Split the graph images into training and testing datasets\n",
        "    train_images = graph_images[:split_index]\n",
        "    test_images = graph_images[split_index:]\n",
        "\n",
        "    # Move the training images to the corresponding activity folder in the training dataset\n",
        "    for image in train_images:\n",
        "        source_path = os.path.join(activity_folder, image)\n",
        "        destination_path = os.path.join(train_activity_folder, image)\n",
        "        shutil.copy(source_path, destination_path)\n",
        "\n",
        "    # Move the testing images to the corresponding activity folder in the testing dataset\n",
        "    for image in test_images:\n",
        "        source_path = os.path.join(activity_folder, image)\n",
        "        destination_path = os.path.join(test_activity_folder, image)\n",
        "        shutil.copy(source_path, destination_path)\n",
        "\n",
        "print(\"Dataset split into training and testing successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufyMijtQU607",
        "outputId": "33294746-affb-40e5-f6c6-cefee59aec64"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'F01_SA01_R01.txt' processed and saved as 'F01_SA01_R01.png'\n",
            "File 'F01_SA15_R01.txt' processed and saved as 'F01_SA15_R01.png'\n",
            "File 'F01_SA03_R01.txt' processed and saved as 'F01_SA03_R01.png'\n",
            "File 'F01_SA12_R01.txt' processed and saved as 'F01_SA12_R01.png'\n",
            "File 'F01_SA07_R01.txt' processed and saved as 'F01_SA07_R01.png'\n",
            "File 'F01_SA19_R02.txt' processed and saved as 'F01_SA19_R02.png'\n",
            "File 'F01_SA10_R01.txt' processed and saved as 'F01_SA10_R01.png'\n",
            "File 'F01_SA14_R01.txt' processed and saved as 'F01_SA14_R01.png'\n",
            "File 'F01_SA18_R01.txt' processed and saved as 'F01_SA18_R01.png'\n",
            "File 'F01_SA09_R01.txt' processed and saved as 'F01_SA09_R01.png'\n",
            "File 'F01_SA19_R03.txt' processed and saved as 'F01_SA19_R03.png'\n",
            "File 'F01_SA20_R01.txt' processed and saved as 'F01_SA20_R01.png'\n",
            "File 'F01_SA02_R01.txt' processed and saved as 'F01_SA02_R01.png'\n",
            "File 'F01_SA16_R01.txt' processed and saved as 'F01_SA16_R01.png'\n",
            "File 'F01_SA04_R01.txt' processed and saved as 'F01_SA04_R01.png'\n",
            "File 'F01_SA13_R01.txt' processed and saved as 'F01_SA13_R01.png'\n",
            "File 'F01_SA22_R01.txt' processed and saved as 'F01_SA22_R01.png'\n",
            "File 'F01_SA05_R01.txt' processed and saved as 'F01_SA05_R01.png'\n",
            "File 'F01_SA23_R01.txt' processed and saved as 'F01_SA23_R01.png'\n",
            "File 'F01_SA06_R01.txt' processed and saved as 'F01_SA06_R01.png'\n",
            "File 'F01_SA21_R01.txt' processed and saved as 'F01_SA21_R01.png'\n",
            "File 'F01_SA11_R01.txt' processed and saved as 'F01_SA11_R01.png'\n",
            "File 'F01_SA08_R01.txt' processed and saved as 'F01_SA08_R01.png'\n",
            "File 'F01_SA19_R01.txt' processed and saved as 'F01_SA19_R01.png'\n",
            "File 'F01_SA17_R01.txt' processed and saved as 'F01_SA17_R01.png'\n",
            "File 'D01_SA17_R01.txt' processed and saved as 'D01_SA17_R01.png'\n",
            "File 'D01_SA13_R01.txt' processed and saved as 'D01_SA13_R01.png'\n",
            "File 'D01_SA18_R01.txt' processed and saved as 'D01_SA18_R01.png'\n",
            "File 'D01_SA07_R01.txt' processed and saved as 'D01_SA07_R01.png'\n",
            "File 'D01_SA16_R01.txt' processed and saved as 'D01_SA16_R01.png'\n",
            "File 'D01_SA23_R01.txt' processed and saved as 'D01_SA23_R01.png'\n",
            "File 'D01_SA04_R01.txt' processed and saved as 'D01_SA04_R01.png'\n",
            "File 'D01_SA20_R01.txt' processed and saved as 'D01_SA20_R01.png'\n",
            "File 'D01_SA22_R01.txt' processed and saved as 'D01_SA22_R01.png'\n",
            "File 'D01_SE03_R01.txt' processed and saved as 'D01_SE03_R01.png'\n",
            "File 'D01_SA09_R01.txt' processed and saved as 'D01_SA09_R01.png'\n",
            "File 'D01_SA19_R01.txt' processed and saved as 'D01_SA19_R01.png'\n",
            "File 'D01_SA01_R01.txt' processed and saved as 'D01_SA01_R01.png'\n",
            "File 'D01_SE02_R01.txt' processed and saved as 'D01_SE02_R01.png'\n",
            "File 'D01_SA14_R01.txt' processed and saved as 'D01_SA14_R01.png'\n",
            "File 'D01_SA06_R01.txt' processed and saved as 'D01_SA06_R01.png'\n",
            "File 'D01_SA12_R01.txt' processed and saved as 'D01_SA12_R01.png'\n",
            "File 'D01_SA08_R01.txt' processed and saved as 'D01_SA08_R01.png'\n",
            "File 'D01_SA10_R01.txt' processed and saved as 'D01_SA10_R01.png'\n",
            "File 'D01_SA11_R01.txt' processed and saved as 'D01_SA11_R01.png'\n",
            "File 'D01_SA05_R01.txt' processed and saved as 'D01_SA05_R01.png'\n",
            "File 'D01_SA02_R01.txt' processed and saved as 'D01_SA02_R01.png'\n",
            "File 'D01_SA03_R01.txt' processed and saved as 'D01_SA03_R01.png'\n",
            "File 'D01_SA15_R01.txt' processed and saved as 'D01_SA15_R01.png'\n",
            "File 'D01_SE01_R01.txt' processed and saved as 'D01_SE01_R01.png'\n",
            "Graph images generated and saved successfully!\n",
            "Dataset split into training and testing successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bSsOXFXuV8yQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}